{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "## Kevon Cambridge\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import matplotlib as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"C:/Users/19545/ADS509 Text Mining/ADS-509-Module-2\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    if num_tokens > 0:\n",
    "        lexical_diversity = num_unique_tokens/num_tokens\n",
    "    else:\n",
    "        return 'num_tokens is 0'\n",
    "    num_characters = len(''.join(tokens))\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(Counter(tokens).most_common(5))\n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: It is beneficial to use assertion statements in our code because it helps us to see if an assuption is true before continuing with your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a3b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cher', 'robyn']\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/19545/ADS509 Text Mining/ADS-509-Module-2/lyrics'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf97ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"88 Degrees\"\\n</td>\n",
       "      <td>\"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"A Different Kind Of Love Song\"\\n</td>\n",
       "      <td>\"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"After All\"\\n</td>\n",
       "      <td>\"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Again\"\\n</td>\n",
       "      <td>\"Again\"\\n\\n\\n\\nAgain evening finds me at your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>\"Alfie\"\\n</td>\n",
       "      <td>\"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"We Dance To The Beat\"\\n</td>\n",
       "      <td>\"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Who's That Girl\"\\n</td>\n",
       "      <td>\"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"With Every Heartbeat\"\\n</td>\n",
       "      <td>\"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"You've Got That Something\"\\n</td>\n",
       "      <td>\"You've Got That Something\"\\n\\n\\n\\nLook at me ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name                              Title  \\\n",
       "0     cher                     \"88 Degrees\"\\n   \n",
       "1     cher  \"A Different Kind Of Love Song\"\\n   \n",
       "2     cher                      \"After All\"\\n   \n",
       "3     cher                          \"Again\"\\n   \n",
       "4     cher                          \"Alfie\"\\n   \n",
       "..     ...                                ...   \n",
       "415  robyn           \"We Dance To The Beat\"\\n   \n",
       "416  robyn          \"Where Did Our Love Go\"\\n   \n",
       "417  robyn                \"Who's That Girl\"\\n   \n",
       "418  robyn           \"With Every Heartbeat\"\\n   \n",
       "419  robyn      \"You've Got That Something\"\\n   \n",
       "\n",
       "                                                Lyrics  \n",
       "0    \"88 Degrees\"\\n\\n\\n\\nStuck in L.A., ain't got n...  \n",
       "1    \"A Different Kind Of Love Song\"\\n\\n\\n\\nWhat if...  \n",
       "2    \"After All\"\\n\\n\\n\\nWell, here we are again\\nI ...  \n",
       "3    \"Again\"\\n\\n\\n\\nAgain evening finds me at your ...  \n",
       "4    \"Alfie\"\\n\\n\\n\\nWhat's it all about, Alfie?\\nIs...  \n",
       "..                                                 ...  \n",
       "415  \"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...  \n",
       "416  \"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...  \n",
       "417  \"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...  \n",
       "418  \"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...  \n",
       "419  \"You've Got That Something\"\\n\\n\\n\\nLook at me ...  \n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the lyrics data\n",
    "artists = []\n",
    "title = []\n",
    "lyrics = []\n",
    "\n",
    "lyrics_data = {}\n",
    "\n",
    "for artist in os.listdir(path):\n",
    "    artist_name = os.path.join(path, artist)\n",
    "    for file in os.listdir(artist_name):\n",
    "        filename = os.path.join(path, artist, file)\n",
    "        with open(filename) as infile:\n",
    "            songs = infile.readlines()\n",
    "        artists.append(artist)\n",
    "        title.append(songs[0])\n",
    "        lyrics.append(''.join(songs))\n",
    "        \n",
    "        #f1 = open(filename, 'r')\n",
    "        #songs = file.replace('.txt', '').split('\\n')\n",
    "        #print(songs[0])\n",
    "\n",
    "lyrics_data = {'Name': artists, 'Title': title, 'Lyrics': lyrics}\n",
    "#lyrics_data\n",
    "df_lyrics = pd.DataFrame(lyrics_data)\n",
    "df_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a178b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_path = 'C:/Users/19545/ADS509 Text Mining/ADS-509-Module-2/twitter/cher_followers_data.txt'\n",
    "robynkonichiwa_path = 'C:/Users/19545/ADS509 Text Mining/ADS-509-Module-2/twitter/robynkonichiwa_followers_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19545\\AppData\\Local\\Temp\\ipykernel_15176\\1627199286.py:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  cher_data = pd.read_csv(cher_path, sep = '\\t',error_bad_lines = False)\n",
      "b'Skipping line 624: expected 7 fields, saw 12\\nSkipping line 17506: expected 7 fields, saw 12\\nSkipping line 104621: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 188924: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 301600: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 429936: expected 7 fields, saw 12\\nSkipping line 444405: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 677792: expected 7 fields, saw 12\\nSkipping line 773482: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 818258: expected 7 fields, saw 12\\nSkipping line 895225: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 955213: expected 7 fields, saw 10\\nSkipping line 994827: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1246039: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1569117: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2127250: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2335031: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2681065: expected 7 fields, saw 10\\n'\n",
      "b'Skipping line 3147696: expected 7 fields, saw 12\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist_name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>AngelxoArts</td>\n",
       "      <td>Angelxo</td>\n",
       "      <td>1424055675030806529</td>\n",
       "      <td>Zacatlan, Puebla, Mexico</td>\n",
       "      <td>29</td>\n",
       "      <td>535</td>\n",
       "      <td>I love chill •Facebook / Instagram / SoundClou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>songsfornikola</td>\n",
       "      <td>johnny</td>\n",
       "      <td>1502717352575651840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>318</td>\n",
       "      <td>books, movies, music, nature &amp; TV shows. OG Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>thibaud_lola</td>\n",
       "      <td>Thibaud Lola</td>\n",
       "      <td>1502407708246478852</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>(Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>KyleSew2112</td>\n",
       "      <td>Kyle S 🌹🇬🇧🇺🇦</td>\n",
       "      <td>3423966821</td>\n",
       "      <td>South East London</td>\n",
       "      <td>1258</td>\n",
       "      <td>3444</td>\n",
       "      <td>This Twitter profile is full of sarcasm and ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>MusiFlo</td>\n",
       "      <td>MusiFlo</td>\n",
       "      <td>3324069364</td>\n",
       "      <td>Canada</td>\n",
       "      <td>470</td>\n",
       "      <td>1706</td>\n",
       "      <td>Flora Youssef - Blogger &amp; Founder Posting revi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Artist_name     screen_name          name                   id  \\\n",
       "0  robynkonichiwa     AngelxoArts       Angelxo  1424055675030806529   \n",
       "1  robynkonichiwa  songsfornikola        johnny  1502717352575651840   \n",
       "2  robynkonichiwa    thibaud_lola  Thibaud Lola  1502407708246478852   \n",
       "3  robynkonichiwa     KyleSew2112  Kyle S 🌹🇬🇧🇺🇦           3423966821   \n",
       "4  robynkonichiwa         MusiFlo       MusiFlo           3324069364   \n",
       "\n",
       "                   location  followers_count  friends_count  \\\n",
       "0  Zacatlan, Puebla, Mexico               29            535   \n",
       "1                       NaN                6            318   \n",
       "2                       NaN                3             69   \n",
       "3         South East London             1258           3444   \n",
       "4                    Canada              470           1706   \n",
       "\n",
       "                                         description  \n",
       "0  I love chill •Facebook / Instagram / SoundClou...  \n",
       "1  books, movies, music, nature & TV shows. OG Sw...  \n",
       "2  (Am)auteur en herbe 🌱 - juriste en paille 🤡 - ...  \n",
       "3  This Twitter profile is full of sarcasm and ra...  \n",
       "4  Flora Youssef - Blogger & Founder Posting revi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the twitter data and adding to dataframe then joining both dataframe\n",
    "\n",
    "#Reading Cher data\n",
    "#col_names = ['screen_names', 'names', 'id', 'location', 'followers_count', 'friends_count', 'description']\n",
    "cher_data = pd.read_csv(cher_path, sep = '\\t',error_bad_lines = False)\n",
    "cher_data['Artist_name'] = 'cher'\n",
    "first_column = cher_data.pop('Artist_name')\n",
    "cher_data.insert(0, 'Artist_name', first_column)\n",
    "\n",
    "#Reading Robynkonichiwa data\n",
    "robynkonichiwa_data = pd.read_csv(robynkonichiwa_path, sep = '\\t', encoding = 'utf-8')\n",
    "robynkonichiwa_data['Artist_name'] = 'robynkonichiwa'\n",
    "first_column = robynkonichiwa_data.pop('Artist_name')\n",
    "robynkonichiwa_data.insert(0, 'Artist_name', first_column)\n",
    "robynkonichiwa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f934091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist_name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>3.515221e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>7.421531e+17</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1.496463e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3.366480e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>csu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>8.391504e+07</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888.0</td>\n",
       "      <td>2891.0</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351834</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>jakegiles</td>\n",
       "      <td>Jake Giles</td>\n",
       "      <td>1.972510e+07</td>\n",
       "      <td>LA</td>\n",
       "      <td>7690.0</td>\n",
       "      <td>2165.0</td>\n",
       "      <td>singer of songs, type 1 diabetic, tired $jakel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351835</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>axelbluhme</td>\n",
       "      <td>Axel Bluhme</td>\n",
       "      <td>1.957376e+07</td>\n",
       "      <td>DK</td>\n",
       "      <td>238.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351836</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>RayRayRay27</td>\n",
       "      <td>Rachael :)</td>\n",
       "      <td>1.991922e+07</td>\n",
       "      <td>Oldham</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>A year to change a life is still a year ✨😌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351837</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>bhandberg</td>\n",
       "      <td>Ben Handberg</td>\n",
       "      <td>1.264246e+07</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>432.0</td>\n",
       "      <td>593.0</td>\n",
       "      <td>Head of Consumer - Mango. Made in Melbourne. R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351838</th>\n",
       "      <td>robynkonichiwa</td>\n",
       "      <td>takemeback</td>\n",
       "      <td>Christine</td>\n",
       "      <td>1.502206e+07</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>182.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>Stand for what is right, even if you stand alone.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4268141 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Artist_name   screen_name          name            id  \\\n",
       "0                 cher        hsmcnp  Country Girl  3.515221e+07   \n",
       "1                 cher    horrormomy          Jeny  7.421531e+17   \n",
       "2                 cher  anju79990584          anju  1.496463e+18   \n",
       "3                 cher  gallionjenna             J  3.366480e+09   \n",
       "4                 cher       bcscomm       bcscomm  8.391504e+07   \n",
       "...                ...           ...           ...           ...   \n",
       "351834  robynkonichiwa     jakegiles    Jake Giles  1.972510e+07   \n",
       "351835  robynkonichiwa    axelbluhme   Axel Bluhme  1.957376e+07   \n",
       "351836  robynkonichiwa   RayRayRay27    Rachael :)  1.991922e+07   \n",
       "351837  robynkonichiwa     bhandberg  Ben Handberg  1.264246e+07   \n",
       "351838  robynkonichiwa    takemeback     Christine  1.502206e+07   \n",
       "\n",
       "              location  followers_count  friends_count  \\\n",
       "0                  NaN           1302.0         1014.0   \n",
       "1                Earth             81.0          514.0   \n",
       "2                  NaN             13.0          140.0   \n",
       "3                  NaN            752.0          556.0   \n",
       "4       Washington, DC            888.0         2891.0   \n",
       "...                ...              ...            ...   \n",
       "351834              LA           7690.0         2165.0   \n",
       "351835              DK            238.0         1565.0   \n",
       "351836          Oldham            762.0         1479.0   \n",
       "351837          Sydney            432.0          593.0   \n",
       "351838      New Jersey            182.0          260.0   \n",
       "\n",
       "                                              description  \n",
       "0                                                     NaN  \n",
       "1                𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜  \n",
       "2               163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡  \n",
       "3                                                     csu  \n",
       "4       Writer @Washinformer @SpelmanCollege alumna #D...  \n",
       "...                                                   ...  \n",
       "351834  singer of songs, type 1 diabetic, tired $jakel...  \n",
       "351835  Dadx2/ Con-Arch/ Photographer/ DK #stemgrønnes...  \n",
       "351836         A year to change a life is still a year ✨😌  \n",
       "351837  Head of Consumer - Mango. Made in Melbourne. R...  \n",
       "351838  Stand for what is right, even if you stand alone.  \n",
       "\n",
       "[4268141 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artist data\n",
    "df_artist_data = pd.concat([cher_data, robynkonichiwa_data])\n",
    "df_artist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ec12066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree = ''.join([i for i in text if i not in punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "#defining function for tokenization\n",
    "def tokenization(text):\n",
    "    return text.split()\n",
    "\n",
    "##defining function for stop removal\n",
    "def remove_stopwords(text):\n",
    "    output = [i for i in text if i not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eedfb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist_name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>description</th>\n",
       "      <th>clean_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher</td>\n",
       "      <td>hsmcnp</td>\n",
       "      <td>Country Girl</td>\n",
       "      <td>3.515221e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1302.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher</td>\n",
       "      <td>horrormomy</td>\n",
       "      <td>Jeny</td>\n",
       "      <td>7.421531e+17</td>\n",
       "      <td>Earth</td>\n",
       "      <td>81.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 &amp; 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜</td>\n",
       "      <td>[𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher</td>\n",
       "      <td>anju79990584</td>\n",
       "      <td>anju</td>\n",
       "      <td>1.496463e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡</td>\n",
       "      <td>[163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher</td>\n",
       "      <td>gallionjenna</td>\n",
       "      <td>J</td>\n",
       "      <td>3.366480e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>752.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>csu</td>\n",
       "      <td>[csu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>bcscomm</td>\n",
       "      <td>8.391504e+07</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>888.0</td>\n",
       "      <td>2891.0</td>\n",
       "      <td>Writer @Washinformer @SpelmanCollege alumna #D...</td>\n",
       "      <td>[writer, washinformer, spelmancollege, alumna,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Artist_name   screen_name          name            id        location  \\\n",
       "0        cher        hsmcnp  Country Girl  3.515221e+07             NaN   \n",
       "1        cher    horrormomy          Jeny  7.421531e+17           Earth   \n",
       "2        cher  anju79990584          anju  1.496463e+18             NaN   \n",
       "3        cher  gallionjenna             J  3.366480e+09             NaN   \n",
       "4        cher       bcscomm       bcscomm  8.391504e+07  Washington, DC   \n",
       "\n",
       "   followers_count  friends_count  \\\n",
       "0           1302.0         1014.0   \n",
       "1             81.0          514.0   \n",
       "2             13.0          140.0   \n",
       "3            752.0          556.0   \n",
       "4            888.0         2891.0   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                      \n",
       "1           𝙿𝚛𝚘𝚞𝚍 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛 𝚘𝚏 𝚖𝚎𝚜𝚜𝚢 𝚋𝚞𝚗𝚜 & 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜   \n",
       "2          163㎝／愛かっぷ💜26歳🍒 工〇好きな女の子💓 フォローしてくれたらDMします🧡   \n",
       "3                                                csu   \n",
       "4  Writer @Washinformer @SpelmanCollege alumna #D...   \n",
       "\n",
       "                                          clean_desc  \n",
       "0                                                 []  \n",
       "1      [𝙿𝚛𝚘𝚞𝚍, 𝚜𝚞𝚙𝚙𝚘𝚛𝚝𝚎𝚛, 𝚘𝚏, 𝚖𝚎𝚜𝚜𝚢, 𝚋𝚞𝚗𝚜, 𝚕𝚎𝚐𝚐𝚒𝚗𝚐𝚜]  \n",
       "2      [163㎝／愛かっぷ💜26歳🍒, 工〇好きな女の子💓, フォローしてくれたらdmします🧡]  \n",
       "3                                              [csu]  \n",
       "4  [writer, washinformer, spelmancollege, alumna,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace Na values with '' \n",
    "df_artist_data['description'] = df_artist_data['description'].replace(np.nan, '')\n",
    "\n",
    "#storing the punctuation free text\n",
    "df_artist_data['clean_desc'] = df_artist_data['description'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "#lowering the description\n",
    "df_artist_data['clean_desc'] = df_artist_data['clean_desc'].apply(lambda x: x.lower())\n",
    "\n",
    "#applying tokenized function to the column\n",
    "df_artist_data['clean_desc'] = df_artist_data['clean_desc'].apply(lambda x: tokenization(x))\n",
    "\n",
    "#applying stopword function\n",
    "df_artist_data['clean_desc'] = df_artist_data['clean_desc'].apply(lambda x:remove_stopwords(x))\n",
    "df_artist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0f22e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>clean_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"We Dance To The Beat\"\\n</td>\n",
       "      <td>\"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...</td>\n",
       "      <td>[dance, beat, dance, beat, dance, beat, dance,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...</td>\n",
       "      <td>[love, go, thoughts, thinkin, used, love, stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Who's That Girl\"\\n</td>\n",
       "      <td>\"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...</td>\n",
       "      <td>[whos, girl, good, girls, pretty, like, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"With Every Heartbeat\"\\n</td>\n",
       "      <td>\"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...</td>\n",
       "      <td>[every, heartbeat, maybe, could, make, right, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"You've Got That Something\"\\n</td>\n",
       "      <td>\"You've Got That Something\"\\n\\n\\n\\nLook at me ...</td>\n",
       "      <td>[youve, got, something, look, im, givin, lovin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name                          Title  \\\n",
       "415  robyn       \"We Dance To The Beat\"\\n   \n",
       "416  robyn      \"Where Did Our Love Go\"\\n   \n",
       "417  robyn            \"Who's That Girl\"\\n   \n",
       "418  robyn       \"With Every Heartbeat\"\\n   \n",
       "419  robyn  \"You've Got That Something\"\\n   \n",
       "\n",
       "                                                Lyrics  \\\n",
       "415  \"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...   \n",
       "416  \"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...   \n",
       "417  \"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...   \n",
       "418  \"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...   \n",
       "419  \"You've Got That Something\"\\n\\n\\n\\nLook at me ...   \n",
       "\n",
       "                                          clean_lyrics  \n",
       "415  [dance, beat, dance, beat, dance, beat, dance,...  \n",
       "416  [love, go, thoughts, thinkin, used, love, stro...  \n",
       "417  [whos, girl, good, girls, pretty, like, time, ...  \n",
       "418  [every, heartbeat, maybe, could, make, right, ...  \n",
       "419  [youve, got, something, look, im, givin, lovin...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create your clean lyrics data here\n",
    "#storing the punctuation free text\n",
    "df_lyrics['clean_lyrics'] = df_lyrics['Lyrics'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "#lowering the description\n",
    "df_lyrics['clean_lyrics'] = df_lyrics['clean_lyrics'].apply(lambda x: x.lower())\n",
    "\n",
    "#applying tokenized function to the column\n",
    "df_lyrics['clean_lyrics'] = df_lyrics['clean_lyrics'].apply(lambda x: tokenization(x))\n",
    "\n",
    "#applying stopword function\n",
    "df_lyrics['clean_lyrics'] = df_lyrics['clean_lyrics'].apply(lambda x:remove_stopwords(x))\n",
    "df_lyrics.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter:\n",
      "Descriptive Stats Cher\n",
      "There are 16148955 tokens in the data.\n",
      "There are 1697994 unique tokens in the data.\n",
      "There are 96043813 characters in the data.\n",
      "The lexical diversity is 0.105 in the data.\n",
      "[('love', 214529), ('im', 139037), ('life', 122900), ('music', 88168), ('de', 73277)]\n",
      "\n",
      "\n",
      "Descriptive Stats Robynkonichiwa\n",
      "There are 1538163 tokens in the data.\n",
      "There are 271325 unique tokens in the data.\n",
      "There are 9397180 characters in the data.\n",
      "The lexical diversity is 0.176 in the data.\n",
      "[('music', 15160), ('love', 11683), ('im', 9052), ('och', 7922), ('life', 7387)]\n",
      "\n",
      "\n",
      "Lyrics\n",
      "Descriptive Stats Cher\n",
      "There are 35916 tokens in the data.\n",
      "There are 3703 unique tokens in the data.\n",
      "There are 172696 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n",
      "[('love', 1004), ('im', 513), ('know', 486), ('dont', 440), ('youre', 333)]\n",
      "\n",
      "\n",
      "Descriptive Stats Robynkonichiwa\n",
      "There are 15227 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73988 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "[('know', 308), ('dont', 301), ('im', 299), ('love', 275), ('got', 251)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15227, 2156, 0.14159059565245943, 73988]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "\n",
    "#getting cher data into a list\n",
    "cher_desc = []\n",
    "cher_lyrics = []\n",
    "cher = df_artist_data[df_artist_data['Artist_name'] == 'cher']\n",
    "cher_song = df_lyrics[df_lyrics['Name'] == 'cher']\n",
    "\n",
    "for desc in cher['clean_desc']:\n",
    "    cher_desc.extend(desc)\n",
    "\n",
    "for lyric in cher_song['clean_lyrics']:\n",
    "    cher_lyrics.extend(lyric)\n",
    "    \n",
    "#getting robynkonichiwa data into a list\n",
    "robynkonichiwa_desc = []\n",
    "robynkonichiwa_lyrics = []\n",
    "robynkonichiwa = df_artist_data[df_artist_data['Artist_name'] == 'robynkonichiwa']\n",
    "robynkonichiwa_song = df_lyrics[df_lyrics['Name'] == 'robyn']\n",
    "\n",
    "for desc in robynkonichiwa['clean_desc']:\n",
    "    robynkonichiwa_desc.extend(desc)\n",
    "    \n",
    "for lyric in robynkonichiwa_song['clean_lyrics']:\n",
    "    robynkonichiwa_lyrics.extend(lyric)\n",
    "    \n",
    "\n",
    "\n",
    "print('Twitter:')\n",
    "print('Descriptive Stats Cher')\n",
    "descriptive_stats(cher_desc)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Descriptive Stats Robynkonichiwa')\n",
    "descriptive_stats(robynkonichiwa_desc)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Lyrics')\n",
    "print('Descriptive Stats Cher')\n",
    "descriptive_stats(cher_lyrics)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Descriptive Stats Robynkonichiwa')\n",
    "descriptive_stats(robynkonichiwa_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If we left stopwords in the top 5 words would either be stopwords such as (is, the, are etc..)\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I do not listen to either artist but cher is a familiar name I've heard from friends and family. robynkonichiwa on the other hand is more unique, I have never heard of robynkonichiwa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_emoji(s):\n",
    "    return(s in emoji.is_emoji('en'))\n",
    "\n",
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "#assert(not name_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten Most Common Emojis For Cher\n",
      "[('❤️', 14731), ('🏳️\\u200d🌈', 14195), ('♥', 10231), ('❤', 9682), ('✨', 8408), ('🌈', 5500), ('🇺🇸', 3737), ('💙', 3708), ('💜', 3509), ('🌊', 3303)]\n",
      "\n",
      "\n",
      "Ten Most Common Emojis For robynkonichiwa\n",
      "[('❤️', 14731), ('🏳️\\u200d🌈', 14195), ('♥', 10231), ('❤', 9682), ('✨', 8408), ('🌈', 5500), ('🇺🇸', 3737), ('💙', 3708), ('💜', 3509), ('🌊', 3303)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "cher_spec = []\n",
    "robynkonichiwa_spec = []\n",
    "\n",
    "for desc in cher['clean_desc']:\n",
    "    for word in desc:\n",
    "        if emoji.is_emoji(word):\n",
    "            cher_spec.append(word)\n",
    "        else:\n",
    "            None\n",
    "cher_num_spec = Counter(cher_spec)\n",
    "print('Ten Most Common Emojis For Cher')\n",
    "print(cher_num_spec.most_common(10))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for desc in robynkonichiwa['clean_desc']:\n",
    "    for word in desc:\n",
    "        if emoji.is_emoji(word):\n",
    "            robynkonichiwa_spec.append(word)\n",
    "        else:\n",
    "            None\n",
    "robynkonichiwa_num_spec = Counter(cher_spec)\n",
    "print('Ten Most Common Emojis For robynkonichiwa')\n",
    "print(robynkonichiwa_num_spec.most_common(10))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten Most Common Hashtags for Cher\n",
      "[('#BLM', 9536), ('#Resist', 6040), ('#BlackLivesMatter', 4675), ('#resist', 3798), ('#FBR', 3240), ('#TheResistance', 2995), ('#blacklivesmatter', 2645), ('#BidenHarris', 2640), ('#Resistance', 1915), ('#VoteBlue', 1877)]\n",
      "Ten Most Common Hashtags for robynkonichiwa\n",
      "[('#BlackLivesMatter', 337), ('#BLM', 307), ('#blacklivesmatter', 208), ('#music', 178), ('#Music', 114), ('#EDM', 86), ('#LGBTQ', 76), ('#TeamFollowBack', 59), ('#blm', 56), ('#travel', 51)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "regexHash = re.compile(r'#[a-zA-Z]+')\n",
    "\n",
    "#Cher's hashtags\n",
    "cher_hashtag = []\n",
    "for tag in cher['description'].astype(str):\n",
    "    hashtag = re.findall(regexHash, tag)\n",
    "    #cher_hashtag.append(hashtag)\n",
    "    cher_hashtag.extend(hashtag)\n",
    "\n",
    "cher_num_hashtag = Counter(cher_hashtag)\n",
    "print('Ten Most Common Hashtags for Cher')\n",
    "print(cher_num_hashtag.most_common(10))\n",
    "\n",
    "\n",
    "#Robyn's hashtags\n",
    "robynkonichiwa_hashtag = []\n",
    "for tag in robynkonichiwa['description'].astype(str):\n",
    "    hashtag = re.findall(regexHash, tag)\n",
    "    robynkonichiwa_hashtag.extend(hashtag)\n",
    "\n",
    "robynkonichiwa_num_hashtag = Counter(robynkonichiwa_hashtag)\n",
    "print('Ten Most Common Hashtags for robynkonichiwa')\n",
    "print(robynkonichiwa_num_hashtag.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c5f3597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>clean_lyrics</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"We Dance To The Beat\"\\n</td>\n",
       "      <td>\"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...</td>\n",
       "      <td>[dance, beat, dance, beat, dance, beat, dance,...</td>\n",
       "      <td>[dance, beat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n</td>\n",
       "      <td>\"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...</td>\n",
       "      <td>[love, go, thoughts, thinkin, used, love, stro...</td>\n",
       "      <td>[love, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"Who's That Girl\"\\n</td>\n",
       "      <td>\"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...</td>\n",
       "      <td>[whos, girl, good, girls, pretty, like, time, ...</td>\n",
       "      <td>[whos, girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"With Every Heartbeat\"\\n</td>\n",
       "      <td>\"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...</td>\n",
       "      <td>[every, heartbeat, maybe, could, make, right, ...</td>\n",
       "      <td>[every, heartbeat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>robyn</td>\n",
       "      <td>\"You've Got That Something\"\\n</td>\n",
       "      <td>\"You've Got That Something\"\\n\\n\\n\\nLook at me ...</td>\n",
       "      <td>[youve, got, something, look, im, givin, lovin...</td>\n",
       "      <td>[youve, got, something]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name                          Title  \\\n",
       "415  robyn       \"We Dance To The Beat\"\\n   \n",
       "416  robyn      \"Where Did Our Love Go\"\\n   \n",
       "417  robyn            \"Who's That Girl\"\\n   \n",
       "418  robyn       \"With Every Heartbeat\"\\n   \n",
       "419  robyn  \"You've Got That Something\"\\n   \n",
       "\n",
       "                                                Lyrics  \\\n",
       "415  \"We Dance To The Beat\"\\n\\n\\n\\nWe dance to the ...   \n",
       "416  \"Where Did Our Love Go\"\\n\\n\\n\\nThoughts about ...   \n",
       "417  \"Who's That Girl\"\\n\\n\\n\\nGood girls are pretty...   \n",
       "418  \"With Every Heartbeat\"\\n\\n\\n\\nMaybe we could m...   \n",
       "419  \"You've Got That Something\"\\n\\n\\n\\nLook at me ...   \n",
       "\n",
       "                                          clean_lyrics  \\\n",
       "415  [dance, beat, dance, beat, dance, beat, dance,...   \n",
       "416  [love, go, thoughts, thinkin, used, love, stro...   \n",
       "417  [whos, girl, good, girls, pretty, like, time, ...   \n",
       "418  [every, heartbeat, maybe, could, make, right, ...   \n",
       "419  [youve, got, something, look, im, givin, lovin...   \n",
       "\n",
       "                 clean_title  \n",
       "415            [dance, beat]  \n",
       "416               [love, go]  \n",
       "417             [whos, girl]  \n",
       "418       [every, heartbeat]  \n",
       "419  [youve, got, something]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning title\n",
    "#storing the punctuation free text\n",
    "df_lyrics['clean_title'] = df_lyrics['Title'].apply(lambda x:remove_punctuation(x))\n",
    "\n",
    "#lowering the description\n",
    "df_lyrics['clean_title'] = df_lyrics['clean_title'].apply(lambda x: x.lower())\n",
    "\n",
    "#applying tokenized function to the column\n",
    "df_lyrics['clean_title'] = df_lyrics['clean_title'].apply(lambda x: tokenization(x))\n",
    "\n",
    "#applying stopword function\n",
    "df_lyrics['clean_title'] = df_lyrics['clean_title'].apply(lambda x:remove_stopwords(x))\n",
    "df_lyrics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five Most Common Words in Song Titles for Cher\n",
      "[('love', 38), ('man', 12), ('song', 11), ('dont', 10), ('come', 7)]\n",
      "\n",
      "\n",
      "Five Most Common Words in Song Titles for Cher\n",
      "[('love', 38), ('man', 12), ('song', 11), ('dont', 10), ('come', 7)]\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "#getting cher data into a list\n",
    "cher_titles = []\n",
    "cher_song = df_lyrics[df_lyrics['Name'] == 'cher']\n",
    "\n",
    "for title in cher_song['clean_title']:\n",
    "    for word in title:\n",
    "        cher_titles.append(word)\n",
    "    \n",
    "#getting robynkonichiwa data into a list\n",
    "robynkonichiwa_titles = []\n",
    "robynkonichiwa_song = df_lyrics[df_lyrics['Name'] == 'robyn']\n",
    "\n",
    "for title in robynkonichiwa_song['clean_title']:\n",
    "    for word in title:\n",
    "        robynkonichiwa_titles.append(word)\n",
    "\n",
    "print('Five Most Common Words in Song Titles for Cher')\n",
    "print(Counter(cher_titles).most_common(5))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "robynkonichiwa_titles = []\n",
    "for title in df_lyrics['clean_title']:\n",
    "    for word in title:\n",
    "        robynkonichiwa_titles.append(word)\n",
    "\n",
    "print('Five Most Common Words in Song Titles for Cher')\n",
    "print(Counter(cher_titles).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Cher              AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robynkonichiwa    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Cher'] * num_replicates + ['robynkonichiwa']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \\s is used for whitespace meaning (space, tabs, newline) and the + means 1 or more. So \\s+ is saying whitespace where there is 1 or more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46ca35",
   "metadata": {},
   "source": [
    "### References:\n",
    "#### For data retrieval:\n",
    "https://www.youtube.com/watch?v=hhjn4HVEdy0 \\n\n",
    "#### For data cleaning:\n",
    "https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b31759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
